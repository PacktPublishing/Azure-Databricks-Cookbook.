{"cells":[{"cell_type":"code","source":["%scala\nimport org.apache.spark.sql.streaming._\n\nspark.streams.addListener(new StreamingQueryListener() {\n  override def onQueryStarted(started: StreamingQueryListener.QueryStartedEvent): Unit = {\n      println(\"Started: \" + started.id)\n  }\n  override def onQueryTerminated(terminated: StreamingQueryListener.QueryTerminatedEvent): Unit = {\n      println(\"Terminated: \" + terminated.id)\n  }\n  override def onQueryProgress(progress: StreamingQueryListener.QueryProgressEvent): Unit = {\n      println(\"Progress: \" + progress.progress)\n  }\n})"],"metadata":{},"outputs":[],"execution_count":1},{"cell_type":"code","source":["%scala\nimport org.apache.spark.eventhubs._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.functions._\n\n// Build connection string with the above information\nval connectionString = ConnectionStringBuilder(\"<Event hub connection string>\")\n  .setEventHubName(\"<Event hub name>\")\n  .build\n\nval customEventhubParameters =\n  EventHubsConf(connectionString)\n  .setMaxEventsPerTrigger(5)\n\nval df = spark.readStream.format(\"eventhubs\").options(customEventhubParameters.toMap).load()\n\ndf.writeStream.outputMode(\"append\").format(\"console\").option(\"truncate\", false).start().awaitTermination()"],"metadata":{},"outputs":[],"execution_count":2}],"metadata":{"name":"Streaming Metrics","notebookId":1795567093467027},"nbformat":4,"nbformat_minor":0}
