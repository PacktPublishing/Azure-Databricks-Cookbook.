{"cells":[{"cell_type":"code","source":["%scala\ndbutils.fs.mount(\n  source = \"wasbs://<Container name>@<Storage account name>.blob.core.windows.net\",\n  mountPoint = \"/mnt/data\",\n  extraConfigs = Map(\"<conf-key>\" -> \"<Storage account key>\"))"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.createAzureStorageSession(AzureNativeFileSystemStore.java:1031)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.initialize(AzureNativeFileSystemStore.java:482)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.initialize(NativeAzureFileSystem.java:1292)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.verifyAzureFileSystem(DBUtilsCore.scala:497)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:446)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.mount(DbfsUtilsImpl.scala:85)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1410223159474767:1)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1410223159474767:47)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1410223159474767:49)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw$$iw.&lt;init&gt;(command-1410223159474767:51)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw.&lt;init&gt;(command-1410223159474767:53)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw.&lt;init&gt;(command-1410223159474767:55)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read.&lt;init&gt;(command-1410223159474767:57)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$.&lt;init&gt;(command-1410223159474767:61)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$.&lt;clinit&gt;(command-1410223159474767)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$eval$.$print(&lt;notebook&gt;:6)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:197)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:197)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:197)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:694)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:647)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:197)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:358)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:272)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:358)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: shaded.databricks.org.apache.hadoop.fs.azure.AzureException: Container data in account databricksdemostorage.blob.core.windows.net not found, and we can't create it using anoynomous credentials, and no credentials found for them in the configuration.\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.connectUsingAnonymousCredentials(AzureNativeFileSystemStore.java:776)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.createAzureStorageSession(AzureNativeFileSystemStore.java:1026)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.AzureNativeFileSystemStore.initialize(AzureNativeFileSystemStore.java:482)\n\tat shaded.databricks.org.apache.hadoop.fs.azure.NativeAzureFileSystem.initialize(NativeAzureFileSystem.java:1292)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.verifyAzureFileSystem(DBUtilsCore.scala:497)\n\tat com.databricks.backend.daemon.dbutils.DBUtilsCore.mount(DBUtilsCore.scala:446)\n\tat com.databricks.dbutils_v1.impl.DbfsUtilsImpl.mount(DbfsUtilsImpl.scala:85)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1410223159474767:1)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1410223159474767:47)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw$$iw$$iw.&lt;init&gt;(command-1410223159474767:49)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw$$iw.&lt;init&gt;(command-1410223159474767:51)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw$$iw.&lt;init&gt;(command-1410223159474767:53)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$$iw.&lt;init&gt;(command-1410223159474767:55)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read.&lt;init&gt;(command-1410223159474767:57)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$.&lt;init&gt;(command-1410223159474767:61)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$read$.&lt;clinit&gt;(command-1410223159474767)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$eval$.$print$lzycompute(&lt;notebook&gt;:7)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$eval$.$print(&lt;notebook&gt;:6)\n\tat line1d15cc6fe3754116918060b0f68e0e8a52.$eval.$print(&lt;notebook&gt;)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat com.databricks.backend.daemon.driver.DriverILoop.execute(DriverILoop.scala:215)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply$mcV$sp(ScalaDriverLocal.scala:197)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:197)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$1.apply(ScalaDriverLocal.scala:197)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExitInternal$.trapExit(DriverLocal.scala:694)\n\tat com.databricks.backend.daemon.driver.DriverLocal$TrapExit$.apply(DriverLocal.scala:647)\n\tat com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:197)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:381)\n\tat com.databricks.backend.daemon.driver.DriverLocal$$anonfun$execute$9.apply(DriverLocal.scala:358)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:235)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:230)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionContext(DriverLocal.scala:49)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:272)\n\tat com.databricks.backend.daemon.driver.DriverLocal.withAttributionTags(DriverLocal.scala:49)\n\tat com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:358)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$tryExecutingCommand$2.apply(DriverWrapper.scala:644)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.tryExecutingCommand(DriverWrapper.scala:639)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.getCommandOutputAndError(DriverWrapper.scala:485)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:597)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInnerLoop(DriverWrapper.scala:390)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:337)\n\tat com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:219)\n\tat java.lang.Thread.run(Thread.java:748)</div>"]}}],"execution_count":1},{"cell_type":"code","source":["%scala\n\nval df = spark.read\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .option(\"delimiter\", \",\")\n  .csv(\"/mnt/data/housing.csv\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">df: org.apache.spark.sql.DataFrame = [longitude: double, latitude: double ... 8 more fields]\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["%scala\nimport org.apache.spark.eventhubs._\n\n// Build connection string with the above information\nval connectionString = ConnectionStringBuilder(\"<Event hub connection string>\")\n  .setEventHubName(\"<Event hub name>\")\n  .build\n\nval customEventhubParameters =\n  EventHubsConf(connectionString)\n  .setMaxEventsPerTrigger(5)\n\nval streaming_df = spark.readStream.format(\"eventhubs\").options(customEventhubParameters.toMap).load()\n\nstreaming_df.join(df)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">import org.apache.spark.eventhubs._\nimport org.apache.spark.sql.streaming.Trigger\nconnectionString: String = Endpoint=sb://mlnetpredict.servicebus.windows.net/;EntityPath=streamingdemo;SharedAccessKeyName=RootManageSharedAccessKey;SharedAccessKey=qRsNbPwNLiNMxjBQY9wIDjKIzmRFkw1LhTEqw0TkC14=\ncustomEventhubParameters: org.apache.spark.eventhubs.EventHubsConf = org.apache.spark.eventhubs.EventHubsConf@4704afa3\nstreaming_df: org.apache.spark.sql.DataFrame = [body: binary, partition: string ... 7 more fields]\nres7: org.apache.spark.sql.DataFrame = [body: binary, partition: string ... 17 more fields]\n</div>"]}}],"execution_count":3}],"metadata":{"name":"Join Stream to Static","notebookId":1410223159474765},"nbformat":4,"nbformat_minor":0}
